[{"title":"Word2Vec详解","date":"2017-04-18T20:37:21.000Z","path":"2017/04/19/Word2Vec详解/","text":"Word2Vec是谷歌2013年开源的自然语言处理的工具包，能够将词语映射到一个向量空间中，在这个空间中，语义相似的词语会靠得比较近，并且还满足一些向量语义的加减操作，比如$\\mathbf{vec}(woman)+\\mathbf{vec}(king)-\\mathbf{vec}(man)\\approx\\mathbf{vec}(queen)$。由于其不错的性能，现在NLP任务中——尤其是那些用深度学习处理的任务——很多都将词语用word2vec转化为向量作为输入，因此应用十分广泛。 1.词向量这篇文章是我在学习peghoty博客和斯坦福cs224d课程中关于word2vec内容时所做的笔记整理而成，点击相应链接可以看到他们的原文。计算机在处理自然语言，首先要将语言数学化；现代计算机很擅长处理向量操作，因此将语言的单位词语表示成向量是一种很好的思路。 1.1one-hot向量最简单的词向量模型是one-hot向量。每个词语在字典中都有唯一的索引，用字典长度的向量来表示每个词语，一个词语对应的向量除了在其索引位置为1外，其余都是0。比如：$$\\begin{aligned} w^{aardvark} = \\begin{bmatrix} 1\\\\0\\\\0\\\\ \\vdots \\\\0 \\end{bmatrix}, w^{a} = \\begin{bmatrix} 0\\\\1\\\\0\\\\ \\vdots \\\\0 \\end{bmatrix}, w^{at} = \\begin{bmatrix} 0\\\\0\\\\1\\\\ \\vdots \\\\0 \\end{bmatrix}, \\dots, w^{zebra} = \\begin{bmatrix} 0\\\\0\\\\0\\\\ \\vdots \\\\1 \\end{bmatrix} \\end{aligned}$$这种方法表示简单，非常容易获得，而且已经能用来做一些事了。但缺点也非常明显：维数太大，很容易造成“维数灾难”；相似词语的向量之间没有对应的关系，无法表现语义关系。比如$$\\begin{aligned} \\left( w^{hotel} \\right)^T w^{motel} = \\left( w^{hotel} \\right)^T w^{cat} = 0 \\end{aligned}$$ 1.2Distributed Representation为了克服one-hot表示方法的局限性，Hinton于1986年引入了distributed representation。基本思想是：通过训练将某种语言中的每一个词映射成一个固定长度的短向量。所有这些向量构成一个向量空间，每个词向量可看作空间中的一点，通过“距离”来表示词语之间的相似性。向量中非零元素会很多，相当于将词的信息分布(distribute)到各分量中了，因此称为“分布式表示”。 这里介绍一个基于窗的共现矩阵的SVD方法。比如有下面这段语料： 123I like deep learning.I like NLP.I enjoy flying. 每个词的共现矩阵就是： 然后对这个矩阵应用SVD分解，将每一列降维成二维向量后，就得到： 可以看到一些语义、位置相似的此会靠得比较近。当然由于语料太少并不明显。word2vec就是一种分布式表示，它有CBOW和Skip-Gram两种模型，层次Softmax和负采样两种方法，结合起来就是4种实现方式。窗口大小为2的CBOW和Skip-Gram模型可以表示为： 2.连续词袋模型连续词袋(CBOW)模型的思想是用周围的词来预测中心词，也就是在已知当前词$w_t$的上下文$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$的前提下预测当前词$w_t$。它的目标函数为对数似然函数：$$\\mathcal L = \\sum_{w \\in \\mathcal C} \\log p \\left( w|Context(w) \\right)$$其中关键是$p\\left( w|Context(w) \\right)$的构造。 2.1层次Softmax模型分为三层：输入层、投影层和输出层。其中： 输入层包含$2c$个词向量$\\mathbf{w_{t-c}},\\mathbf{w_{t-c+1}}, \\cdots, \\mathbf{w_{t+c-1}}, \\mathbf{w_{t+c}} \\in \\mathbb R^m$ 投影层将这些向量求和累加，即：$\\mathbf x_w=\\sum_{i=-c,i\\neq 0}^c \\mathbf{w_{t+i}} \\in \\mathbb R^m$ 输出层对应一棵以语料中出现的词为叶节点，以各词在语料中出现次数为为权值构造的Huffman树。这棵Huffman树中，叶子节点有$N=|\\mathcal D|$个，分别对应$\\mathcal D$中的词，非叶节点为$N-1$个。 层次softmax是word2vec中用于提高性能的关键技术。考虑Huffman树的叶子节点，假设其对应字典$\\mathcal D$中的词$w$，记： $p^w$：从根节点出发到达$w$对应子结点的路径； $l^w$：路径$p^w$中包含的结点个数； $p_1^w, p_2^w,\\cdots,p_{l^w}^w$：路径$p^w$中$l^w$个结点，其中$p_1^w$表示根节点，$p_{l^w}^w$表示词$w$对应的结点。 $d_2^w, d_3^w, \\cdots, d_{l^w}^w \\in {0,1}$：词$w$的Huffman编码，由$l^w-1$位构成。 $\\theta_1^w, \\theta_2^w, \\cdots, \\theta_{l^w-1}^w$：路径$p^w$中非叶结点对应的向量。 若有句子“我喜欢看巴西足球世界杯”，考虑词语$w$为“足球”，假设通过语料库建立的Huffman树如下图所示： 其中红色边连结的5个结点即为路径$p^w$，长度$l^w=5$，可以看出“足球”的Huffman编码为$1001$。从图中可以看出，从根节点到达“足球”这个叶节点，中间共经历4次分支，而每一次分支可视为一次二分类。这里，记：$$\\begin{aligned} Label(p_i^w) = 1 - d_i^w, i = 2,3,\\cdots,l^w \\end{aligned}$$即将一个节点进行分类时，分到左边为负类，标记为1；分到右边为正类，标记为0。由logistic回归，一个节点被分为正类的概率是$$\\begin{aligned} \\sigma \\left( \\mathbf x_w^T\\theta \\right) = \\frac{1}{1+e^{-\\mathbf x_w^T\\theta}} \\end{aligned}$$其中$\\theta$为待定参数，也就是上面非叶结点对应的向量。对于从根节点到达“足球”所经历的4次二分类，每次分类结构的概率就是： 第1次：$p\\left( d_2^w|\\mathbf x,\\theta_1^w \\right)=1-\\sigma\\left( \\mathbf x_w^T\\theta_1^w \\right)$； 第2次：$p\\left( d_3^w|\\mathbf x,\\theta_2^w \\right)=\\sigma\\left( \\mathbf x_w^T\\theta_2^w \\right)$ 第3次：$p\\left( d_4^w|\\mathbf x,\\theta_3^w \\right)=\\sigma\\left( \\mathbf x_w^T\\theta_3^w \\right)$ 第4次：$p\\left( d_5^w|\\mathbf x,\\theta_4^w \\right)=1-\\sigma\\left( \\mathbf x_w^T\\theta_4^w \\right)$ 于是，就有$$\\begin{aligned} p\\left( 足球|Context(足球) \\right) = \\prod_{j=2}^5 p\\left( d_j^w|\\mathbf x_w,\\theta_{j-1}^w \\right) \\end{aligned}$$因此，Hierarchical Softmax的基本思想就是：对于辞典$\\mathcal D$中的任意词$w$，Huffman树中必唯一存在一条你从根节点到$w$所对应的的路径$p^w$，其上面存在$l^w-1$个分支，每个分支可视为一次二分类，每一次分类就产生一个概率，将这些概率相乘，即为所需的$p(w|Context(w))$，一般可写为：$$\\begin{aligned} p(w|Context(w)) = \\prod_{j=2}^{l^w} p(d_j^w|\\mathbf x_w, \\theta_{j-1}^w) \\end{aligned}$$其中$$\\begin{aligned} p\\left(d_j^w|\\mathbf x_w, \\theta_{j-1}^w\\right) = \\left[\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right]^{1-d_j^w} \\cdot \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right]^{d_j^w} \\end{aligned}$$将上式代入似然公式中，可得：$$\\begin{aligned} \\mathcal L &amp;= \\sum_{w\\in \\mathcal C} \\log \\prod_{j=2}^{l^w}\\left\\{ \\left[\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right]^{1-d_j^w} \\cdot \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right]^{d_j^w}\\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{j=2}^{l^w} \\left\\{ \\left(1-d_j^w\\right) \\cdot \\log\\left[\\sigma\\left(\\mathbf x_w^T \\theta_{j-1}^w\\right)\\right]+d_j^w\\cdot \\log\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right] \\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{j=2}^{l^w} \\mathcal L(w,j) \\end{aligned}$$其中$\\mathcal L(w,j)= \\left(1-d_j^w\\right) \\cdot \\log\\left[\\sigma\\left(\\mathbf x_w^T \\theta_{j-1}^w\\right)\\right]+d_j^w\\cdot \\log\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right]$。用SGD（随机梯度下降）来求解$\\mathcal L$的最大值，就需要求出$\\mathcal L$关于向量$\\mathbb x_w, \\theta_{j-1}^w, w\\in \\mathcal C, j=2,3,\\cdots,l^w$。首先考虑$\\mathcal L(w,j)$关于$\\theta_{j-1}^w$的梯度：$$\\begin{aligned} \\frac{\\partial\\mathcal L(w,j)}{\\partial\\theta_{j-1}^w} &amp;= \\frac{\\partial}{\\partial\\theta_{j-1}^w}\\left\\{ \\left(1-d_j^w\\right) \\cdot \\log\\left[\\sigma\\left(\\mathbf x_w^T \\theta_{j-1}^w\\right)\\right]+d_j^w\\cdot \\log\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right] \\right\\} \\\\ &amp;= \\left( 1-d_j^w \\right) \\left[ 1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right) \\right]\\mathbf x_w-d_j^w\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\mathbf x_w \\\\ &amp;= \\left\\{ \\left(1-d_j^w\\right)\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right)\\right]-d_j^w\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right) \\right\\}\\mathbf x_w \\\\ &amp;= \\left[ 1-d_j^w-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right) \\right]\\mathbf x_w \\end{aligned}$$因此$\\theta_{j-1}^w$的更新公式为：$$\\begin{aligned} \\theta_{j-1}^w := \\theta_{j-1}^w + \\eta\\left[ 1-d_j^w-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right) \\right]\\mathbf x_w \\end{aligned}$$其中$\\eta$表示学习率。然后考虑$\\mathcal L(w,j)$关于$\\mathbf x_w$的梯度。观察上面的等式可看出$\\mathcal L(w,j)$关于变量$\\mathbf x_w$和$\\theta_{j-1}^w$是对称的（即两者可以交换位置），因此相应梯度$\\frac{\\partial\\mathcal L(w,j)}{\\partial\\mathbf x_w}$也只需在$\\frac{\\partial\\mathcal L(w,j)}{\\partial\\theta_{j-1}^w}$的基础上对这两个向量交换位置即可，$$\\begin{aligned} \\frac{\\partial \\mathcal L(w,j)}{\\partial \\mathbf x_w} = \\left[ 1-d_j^w-\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right) \\right]\\theta_{j-1}^w \\end{aligned}$$这里有个问题是我们最终的目的是求字典$\\mathcal D$中每个词的词向量，而这里的$\\mathbf x_w$表示$Context(w)$中各词词向量的累加，word2vec通过$$\\begin{aligned} \\mathbf v(\\widetilde w) := \\mathbf v(\\widetilde w)+\\eta\\sum_{j=2}^{l^w}\\frac{\\partial \\mathcal L(w,j)}{\\partial \\mathbf x_w}, \\widetilde w \\in Context(w) \\end{aligned}$$来使用$\\frac{\\partial \\mathcal L(w,j)}{\\partial \\mathbf x_w}$对$\\mathbf v(\\widetilde{w})\\in Context(w)$进行更新，也就是将$\\sum_{j=2}^{l^w}\\frac{\\partial \\mathcal L(w,j)}{\\partial \\mathbf x_w}$贡献到每一个$Context(w)$的每一个词的词向量上面。以$\\left(Context(w),w\\right)$为例给出CBOW模型采用SGD更新各参数的伪代码：$$\\begin{aligned} &amp; 1.\\ \\mathbf e=0 \\\\ &amp; 2.\\ \\mathbf x_w = \\sum_{u\\in Context(w)} \\mathbf v(u) \\\\ &amp; 3.\\ \\text{For}\\ j=2:l^w\\ \\text{Do} \\\\ &amp;\\ \\ \\ \\ \\{ \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.1\\ q=\\sigma\\left(\\mathbf x_w^T\\theta_{j-1}^w\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.2\\ g=\\eta\\left(1-d_j^w-q\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.3\\ \\mathbf e:=\\mathbf e+g\\theta_{j-1}^w \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.4\\ \\theta_{j-1}^w:=\\theta_{j-1}^w+g\\mathbf x_w \\\\ &amp;\\ \\ \\ \\ \\} \\\\ &amp; 4.\\ \\text{For}\\ u \\in Context(w)\\ \\text{Do} \\\\ &amp;\\ \\ \\ \\ \\{ \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf v(u):=\\mathbf v(u)+\\mathbf e \\\\ &amp;\\ \\ \\ \\ \\} \\\\ \\end{aligned}$$ 2.2负采样方法负采样（Negative Sampling，简称为NEG）用来提高训练速度并改善词向量的质量，它利用随机负采样，能大幅提高性能，可作为层次Softmax的一种替代。在CBOW中已知上下文$Context(w)$预测中心词$w$，因此$w$就是一个正样本，而其他词就是负样本。 假定已有现在已有一个关于$w$的负样本子集$NEG(w)\\neq\\emptyset$，且对$\\forall \\widetilde w \\in \\mathcal D$，定义：$$\\begin{aligned} L^w\\left(\\widetilde w\\right) = \\begin{cases} 1, &amp; \\widetilde w = w; \\\\[2ex] 0, &amp; \\widetilde w \\neq w. \\end{cases} \\end{aligned}$$对于给定正样本$\\left(Context(w), w\\right)$我们希望最大化$$\\begin{aligned} g(w) &amp;= \\prod_{u\\in \\{w\\}\\cup NEG(w)} p\\left(u|Context(w)\\right) \\\\ &amp;= \\prod_{u\\in \\{w\\}\\cup NEG(w)} \\left[\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]^{L^w(u)} \\cdot \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]^{1-L^w(u)} \\\\ &amp;= \\sigma\\left(\\mathbf x_w^T\\theta^w\\right) \\prod_{u\\in NEG(w)} \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right] \\end{aligned}$$其中$\\mathbf x_w$仍表示$Context(w)$中各词词向量之和；而$\\theta^u \\in \\mathbb R^m$表示待训练的词$u$对应的一个向量；$\\sigma\\left(\\mathbf x_w^T\\theta^w\\right)$为上下文为$Context(w)$时预测中心词为$w$的概率；而$\\sigma\\left(\\mathbf x_w^T\\theta^u\\right), u \\in NEG(w)$表示上下文为$Context(w)$时预测中心词为$u$的概率；因此最大化$g(w)$就相当于最大化$\\sigma\\left(\\mathbf x_w^T\\theta^w\\right)$的同时最小化所有$\\sigma\\left(\\mathbf x_w^T\\theta^u\\right), u \\in NEG(w)$，也即增大正样本概率的同时降低负样本的概率。对于给定语料库$\\mathcal C$，函数的目标函数可以写为：$$\\begin{aligned} \\mathcal L &amp;= \\log \\prod_{w\\in \\mathcal C} g(w) \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\log g(w) \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\log \\prod_{u\\in \\{w\\}\\cup NEG(w)} \\left\\{ \\left[\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]^{L^w(u)} \\cdot \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]^{1-L^w(u)} \\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{u\\in \\{w\\}\\cup NEG(w)} \\left\\{ L^w(u)\\cdot\\log\\left[\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right] + \\left[1-L^w(u)\\right]\\cdot\\log\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right] \\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{u\\in \\{w\\}\\cup NEG(w)} \\mathcal L(w,u) \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\left\\{ log\\left[\\sigma\\left(\\mathbf x_w^T\\theta^w\\right)\\right]+\\sum_{u\\in NEG(w)} log\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]\\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\left\\{ log\\left[\\sigma\\left(\\mathbf x_w^T\\theta^w\\right)\\right]+\\sum_{u\\in NEG(w)} log\\left[\\sigma\\left(-\\mathbf x_w^T\\theta^u\\right)\\right]\\right\\} \\end{aligned}$$同样使用SGD，$\\mathcal L(w,u)$关于$\\theta^u$的梯度为：$$\\begin{aligned} \\frac{\\partial \\mathcal L(w,u)}{\\partial \\theta^u} &amp;= \\frac{\\partial}{\\partial \\theta^u} \\left\\{ L^w(u)\\cdot\\log\\left[\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right] + \\left[1-L^w(u)\\right]\\cdot\\log\\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right] \\right\\} \\\\ &amp;= L^w(u) \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]\\mathbf x_w-\\left[1-L^w(u)\\right]\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\mathbf x_w \\\\ &amp;= \\left\\{ L^w(u) \\left[1-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right)\\right]-\\left[1-L^w(u)\\right] \\sigma\\left(\\mathbf x_w^T\\theta^u\\right) \\right\\}\\mathbf x_w \\\\ &amp;= \\left[ L^w(u)-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right) \\right]\\mathbf x_w \\end{aligned}$$于是$\\theta^u$的更新公式为：$$\\begin{aligned} \\theta^u := \\theta^u + \\eta\\left[ L^w(u)-\\sigma\\left(\\mathbf x\\_w^T\\theta^u\\right) \\right]\\mathbf x_w \\end{aligned}$$同样利用$\\mathcal L(w,u)$ 中$\\mathbf x_w$和$\\theta^u$的对称性，有：$$\\begin{aligned} \\frac{\\partial \\mathcal L(w,u)}{\\partial \\mathbf x_w} =\\left[ L^w(u)-\\sigma\\left(\\mathbf x_w^T\\theta^u\\right) \\right]\\theta^u \\end{aligned}$$于是利用$\\frac{\\partial \\mathcal L(w,u)}{\\partial \\mathbf x_w}$可得$\\mathbf v(\\widetilde w), \\widetilde w\\in Context(w)$的更新公式为：$$\\begin{aligned} \\mathbf v(\\widetilde w) := \\mathbf v(\\widetilde w) + \\eta\\sum_{u\\in \\{w\\}\\cup NEG(w)} \\frac{\\partial \\mathcal L(w,u)}{\\partial \\mathbf x_w},\\ \\ \\widetilde w\\in Context(w) \\end{aligned}$$以样本$\\left(Context(w),w\\right)$为例给出负采样的CBOW模型的伪代码：$$\\begin{aligned} &amp; 1.\\ \\mathbf e=0 \\\\ &amp; 2.\\ \\mathbf x_w = \\sum_{u\\in Context(w)} \\mathbf v(u) \\\\ &amp; 3.\\ \\text{For}\\ u=\\{w\\}\\cup NEG(w)\\ \\text{Do} \\\\ &amp;\\ \\ \\ \\ \\{ \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.1\\ q=\\sigma\\left(\\mathbf x_w^T\\theta^u\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.2\\ g=\\eta\\left(L^w(u)-q\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.3\\ \\mathbf e:=\\mathbf e+g\\theta^u \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ 3.4\\ \\theta^u:=\\theta^u+g\\mathbf x_w \\\\ &amp;\\ \\ \\ \\ \\} \\\\ &amp; 4.\\ \\text{For}\\ u \\in Context(w)\\ \\text{Do} \\\\ &amp;\\ \\ \\ \\ \\{ \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf v(u):=\\mathbf v(u)+\\mathbf e \\\\ &amp;\\ \\ \\ \\ \\} \\\\ \\end{aligned}$$ 3.Skip-Gram模型跳元模型(Skip-Gram)与CBOW相反，利用中心词来预测上下文，即已知中心词$w_t$的情况下预测上下文$w_{t-2}, w{t-1},w_{t+1},w_{t+2}$。优化的目标函数是$$\\begin{aligned} \\mathcal L = \\sum_{w \\in \\mathcal C} \\log p \\left( Context(w)|w \\right) \\end{aligned}$$ 3.1Hierachical Softmax方法模型依然是三层：输入层、投影层和输出层。 输出层只包含当前样本的中心词$w$的词向量$\\mathbf v(w) \\in \\mathbb R^m$； 投影层是恒等投影，把$\\mathbf v(w)$投影到$\\mathbf v(w)$； 输出层也是一棵Huffman树。 这里的条件概率函数就是：$$\\begin{aligned} p\\left(Context(w)|w\\right) = \\prod_{u\\in Context(w)} p(u|w) \\end{aligned}$$仿照上面的思想：$$\\begin{aligned} p(u|w) = \\prod_{j=2}^{l^u} p\\left( d_j^u|\\mathbf v(w),\\theta_{j-1}^u \\right) \\end{aligned}$$其中：$$\\begin{aligned} p\\left( d_j^u|\\mathbf v(w),\\theta_{j-1}^u \\right) = \\left[\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]^{1-d_j^u} \\cdot \\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]^{d_j^u} \\end{aligned}$$于是得到对数似然函数为：$$\\begin{aligned} \\mathcal L &amp;= \\sum_{w\\in \\mathcal C} \\log \\prod_{u\\in Context(w)} \\prod_{j=2}^{l^u} \\left\\{ \\left[\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]^{1-d_j^u} \\cdot \\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]^{d_j^u} \\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{u\\in Context(w)} \\sum_{j=2}^{l^u} \\left\\{ \\left(1-d_j^u\\right)\\cdot \\log\\left[\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]+d_j^u\\log\\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right] \\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{u\\in Context(w)} \\sum_{j=2}^{l^u} \\mathcal L(w,u,j) \\end{aligned}$$这就是Skip-Gram模型的目标函数，其中：$$\\begin{aligned} \\mathcal L(w,u,j)= \\left(1-d_j^u\\right)\\cdot \\log\\left[\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]+d_j^u\\log\\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right] \\end{aligned}$$同样用SGD求解最优值，考虑$\\mathcal L(w,u,j)$关于$\\theta_{j-1}^u$的梯度：$$\\begin{aligned} \\frac{\\partial \\mathcal L(w,u,j)}{\\partial \\theta_{j-1}^u} &amp;= \\frac{\\partial}{\\partial \\theta_{j-1}^u} \\left\\{ \\left(1-d_j^u\\right)\\cdot \\log\\left[\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]+d_j^u\\log\\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right] \\right\\} \\\\ &amp;= \\left( 1-d_j^u \\right)\\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]\\mathbf v(w) - d_j^u\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\mathbf v(w) \\\\ &amp;= \\left\\{ \\left(1-d_j^u\\right) \\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right] -d_j^u\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right) \\right\\}\\mathbf v(w) \\\\ &amp;= \\left[1-d_j^u-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]\\mathbf v(w) \\end{aligned}$$于是$\\theta_{j-1}^u$的更新公式可写为：$$\\begin{aligned} \\theta_{j-1}^u := \\theta_{j-1}^u + \\eta\\left[1-d_j^u-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]\\mathbf v(w) \\end{aligned}$$然后考虑$\\mathcal L(w,u,j)$关于$\\mathbf v(w)$的梯度，同样利用$\\mathcal L(w,u,j)$中$\\mathbf v(w)$和$\\theta_{j-1}^u$的对称性，有$$\\begin{aligned} \\frac{\\partial \\mathcal L(w,u,j)}{\\partial \\mathbf v(w)}=\\left[1-d_j^u-\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right)\\right]\\theta_{j-1}^u \\end{aligned}$$于是$\\mathbf v(w)$的更新公式为：$$\\begin{aligned} \\mathbf v(w) := \\mathbf v(w) + \\eta\\sum_{u\\in Context(w)}\\sum_{j=2}^{l^u} \\frac{\\partial \\mathcal L(w,u,j)}{\\partial \\mathbf v(w)} \\end{aligned}$$以样本$\\left(w,Context(w)\\right)$给出Skip-Gram模型采用SGD更新各参数伪代码：$$\\begin{aligned} &amp; \\mathbf e=0 \\\\ &amp; \\text{For}\\ u\\in Context(w)\\ \\text{Do} \\\\ &amp;\\{ \\\\ &amp;\\ \\ \\ \\ \\text{For}\\ j=2:l^u\\ \\text{Do}\\\\ &amp;\\ \\ \\ \\ \\{\\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ 1.\\ q=\\sigma\\left(\\mathbf v(w)^T\\theta_{j-1}^u\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ 2.\\ g=\\eta\\left(1-d_j^u-q\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ 3.\\ \\mathbf e:=\\mathbf e+g\\theta_{j-1}^u \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ 4.\\ \\theta_{j-1}^u:=\\theta_{j-1}^u+g\\mathbf v(w) \\\\ &amp;\\ \\ \\ \\ \\}\\\\ &amp;\\} \\\\ &amp;\\mathbf v(w) := \\mathbf v(w) + \\mathbf e \\end{aligned}$$ 3.2负采样方法类似于层次Softmax对于一个给定样本，我们希望最大化$$\\begin{aligned} g(w) &amp;= \\prod_{u \\in Context(w)} g(u) \\\\ &amp;= \\prod_{u \\in Context(w)} \\prod_{u\\in \\{w\\}\\cup NEG(w)} p(z|w) \\\\ &amp;= \\prod_{u \\in Context(w)} \\prod_{u\\in \\{w\\}\\cup NEG(w)} \\left[\\sigma\\left(\\mathbf v(w)^T\\theta^z\\right)\\right]^{L^u(z)} \\cdot \\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta^z\\right)\\right]^{1-{L^u(z)}} \\end{aligned}$$最终的目标函数是：$$\\begin{aligned} \\mathcal L &amp;= \\log \\prod_{w\\in \\mathcal C} g(w) \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\log \\prod_{u \\in Context(w)} \\prod_{u\\in \\{w\\}\\cup NEG(w)} p(z|w) \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{u\\in Context(w)} \\sum_{z\\in \\{u\\}\\cup NEG(u)} \\log \\left\\{ \\left[\\sigma\\left(\\mathbf v(w)^T\\theta^z\\right)\\right]^{L^u(z)} \\cdot \\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta^z\\right)\\right]^{1-{L^u(z)}} \\right\\} \\\\ &amp;= \\sum_{w\\in \\mathcal C} \\sum_{u\\in Context(w)} \\sum_{z\\in \\{u\\}\\cup NEG(u)} \\left\\{ L^u(z)\\cdot \\log\\left[\\sigma\\left(\\mathbf v(w)^T\\theta^z\\right)\\right]+\\left[1-L^u(z)\\right] \\cdot \\log\\left[1-\\sigma\\left(\\mathbf v(w)^T\\theta^z\\right)\\right] \\right\\} \\end{aligned}$$按照上面同样的方法可以求出相应变量的梯度。","tags":[{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"Git学习笔记1—入门","date":"2017-04-12T13:53:53.000Z","path":"2017/04/12/Git学习笔记1—入门/","text":"Git是一个分布式的版本控制系统，这篇文章会介绍一些基础的用法，包括版本库的创建、添加文件、回退、远程同步等。 版本系统可以自动地记录一个项目每次文件的改动，可以方便地实现与他人进行项目协作、管理和传输文件。 Git是一个分布式的版本控制系统，每台机器里面都有完整的版本库，因此无需联网即可工作，也具有很高的安全性；在协同工作时仅需将各自修改推送给对方，不过一般也会有一台电脑充当“中央服务器的角色”，以方便交换修改。 1.创建版本库进入所需控制项目的根目录中（也可以是空目录），执行命令git init即可创建一个版本库。通过ls -ah可以发现目录下面会多一个.git目录，Git用它来追踪管理版本库，务必不要手动修改。 1.1添加文件要添加文件到目录（或者子目录）中， 第一步先使用git add命令将文件添加到仓库中，比如：1git add readme.md 第二步使用git commit命令将文件提交到仓库，-m参数后面为本次提交的说明：1git commit -m \"Write a readme file\" 需注意的是commit一次可以提交很多文件，因此可以多次add文件后再用commit一次提交：123git add readme.mdgit a.h b.hgit commit -m \"add three files\" 1.2查看动态要掌握仓库的当前状态，使用git status命令，它会显示文件的修改以及准备提交的状态；若要查看某个文件修改具体内容，使用git diff命令，它会显示这个文件修改了但还没有提交的情况，比如修改了readme.md文件；最后使用git commit命令提交：123git statusgit diff readme.mdgit commit -m \"change the readme.md\" 2.版本回退每次commit，Git就会保存文件修改的一个“快照”。当误操作之后还可以从最近的一个commit恢复。若要看项目修改的记录，使用git log命令，加上--pretty=oneline参数，会精简地显示信息：12git loggit log --pretty=oneline 但log命令至少会显示commit id（版本号）和每次的提交说明。commit id是SHA1计算出来的一个非常大的数字，用16进制表示。 2.1版本回退在git中，使用HEAD表示当前版本，上一个版本是HEAD^，上上个版本是HEAD^^，而往上100个版本则是HEAD~100。要回退到上个版本，使用git reset命令：1git reset --hard HEAD^ 如果想返回刚才的版本，可以通过git reflog命令根据提交说明找到那个版本的commit id，假设是4325678...,就可以回去：1git reset --hard 4325678 版本号写前几位即可（git reflog命令也只显示7位），Git会自动去找。Git回退的速度非常快，因其内部的HEAD指针指向当前的版本，回退的时候仅需将HEAD指向前要回退的版本，并更新工作区。 2.2工作区与缓存区Git有工作区和暂存区两个概念。工作区就是能够看到的目录；隐藏的.git目录是版本库，其中最重要的是被称为stage（或index）的暂存区，Git自动为我们创建的master分支，以及指向master的一个指针HEAD。往仓库里添加文件时，git add会将文件修改添加到暂存区，而git commit则会将暂存区的所有内容提交到当前分支（所以每次修改文件后必须要有git add和git commit两步）。 2.3管理修改必须要弄清的一点是Git跟踪并管理的并非文件，而是修改（比如文件的创建或内容的修改）。对刚才的readme.md文件，123456do some chages and save for readme.mdgit add readme.mddo some changes and save for readme.md againgit commit -m \"do some chages\"git statusgit diff HEAD -- readme.md 通过此时会发现第二次修改并未被提交。回顾一下操作过程是： 第一次修改 –&gt; git add –&gt; 第二次修改 –&gt; git commit 因为Git管理的是修改，在使用git add之后，工作区的第一次修改放入暂存区；但工作区的第二次修改并未放入暂存区，所以git commit只是将暂存区的修改提交了。也就是说每次修改，不add到暂存区，就不会加入到commit中。 2.4撤销修改可以使用git checkout -- filename命令来丢弃工作区d的修改，它会将工作区的这个文件回复到最近一次git commit或git add的状态；使用git reset HEAD filename则可以将add进暂存区的修改撤销掉。 2.5删除文件在工作区中使用bash的rm命令删除某个文件filename后： 此时如果确实要从版本库中删除它，可以使用git rm命令和git commit命令：12git rm filenamegit commit -m \"delete some redundant files\" 若是想恢复到最新版本，可以使用：1git checkout -- filename git checkout其实就是用版本库里的版本替换工作区的版本。 3.远程仓库Git是分布式版本控制系统，同一个Git库可以分布到不同机器上，且每台机器都一样并无主次之分。不过实际中为方便代码的交换，会使用一台机器作为Git服务器，而GitHub就是专门提供Git仓库托管服务的。 3.1设置连接注册Github帐号后，本地仓库与Github仓库之间的传输通过SSH加密，需要一些设置： 1)创建SSH Key。在shell中输入：1ssh-keygen -t rsa -C \"youremail.example.com\" 一路回车，即可在主目录中找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，即SSH Key的密钥对，前者为私钥，后者为公钥。 2)登陆Github，打开”settings”，”SSH and GPG keys”，选择”New SSH key”，随意填title，在Key文本框里粘贴id_rsa.pub内容。 3.2添加远程库将本地库与远程库同步。 1)首先登陆Github，点击”Create a new repo”，填入项目名称，点击”Create repository”。 2)然后在本地仓库根目录下执行1git remote add origin git@github.com:yourname/yourrepo 添加后远程仓库的名字就是origin，这是Git默认叫法，也可以改成其他。 3)用git push命令将本地库的所有内容推送到远程，即将当前分支master推送到远程。1git push -u origin master 由于远程库是空的，因此在第一次推送master分支时加上-u参数，Git还会将本地master分支与远程的master分支关联起来，以后推送就可以使用命令1git push origin master 第一次使用Git的push或clone命令连接Github时会出现警告，没有关系。 3.3从远程库克隆使用git clone克隆一个本地库：1git clone git@github.com:yourname/yourrepo.git Git支持多种协议，默认的git://使用ssh，但也可以使用https等其他协议：1git clone https://github.com/yourname/yourrepo 但通过ssh支持的原生git协议速度最快。","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"计算图上的微积分—反向传播","date":"2017-04-10T16:08:21.000Z","path":"2017/04/11/计算图上的微积分—反向传播/","text":"反向传播算法是在机器学习中应用十分广泛的算法，但很多人并未真正理解其重要性。这里会通过计算图来比较前向算法和反向算法的差异，从而展现反向传播的强大和优越之处。 1. 计算图计算图是很好的思考数学表达式的方法。例如式子$e=(a+b)*(b+1)$。有加法和乘法三个运算符，引入俩个中间变量$c$和$d$，这样每个函数的输出就都有一个变量了，于是有：$$c = a + b \\\\d = b + 1 \\\\e = c * d$$要产生计算图，使每个这些运算符和输入变量到节点中去。当一个节点的值是另一个节点的输入时，用一个箭头从这个节点指向另一个： 这种图在计算机科学中一直出现，尤其是谈到函数式程序的时候，与依赖图和调用图的概念非常接近，也同样是流行的深度学习框架Theano的核心概念。 可以设定输入变量为某些特定的值并通过计算图的节点来评估表达式。比如，设$a=2, b=1$： 2. 计算图上的求导要理解计算图中的求导，关键是理解边上的求导。如果$a$直接影响$c$，那它在什么程度影响呢？可以通过偏导数来评估，要理解这个图中的偏导，需要加法规则和乘法规则：$$\\frac{\\partial}{\\partial a}(a+b) = \\frac{\\partial a}{\\partial a} + \\frac{\\partial b}{\\partial a} = 1 \\\\\\frac{\\partial}{\\partial u}{uv} = u \\frac{\\partial v}{\\partial u} + v \\frac{\\partial u}{\\partial u} = v$$然后每个边的求导就是： 那非直连的两个节点是如何相互影响的呢？考虑$e$如何受$a$的影响。若以速度1改变$a$，$c$也会以速度1改变；接下来，$c$以1的速度改变会导致$e$以速度2变化。所以对于$a$，$e$以$1*2$的速率变化。 通用规则是将一个节点到另一个节点所有可能的路径相加，并将路径上的每条边的导数相乘起来。例如，求$e$对应于$b$的导数：$$\\frac{\\partial e}{\\partial b} = 1*2 + 1*3$$这解释了$b$如何通过$c$和$d$影响$e$。“所有路径相加”的规则是另一种考虑多元链式法则的方法。 3. 分解路径“所有路径相加”在可能路径数量上很容易导致组合爆炸的问题。 上面的图表中有三条从$X$到$Y$的路径，三条从$Y$到$Z$。若想要通过所有路径相加得到导数$\\frac{\\partial Z}{\\partial X}$，需要加上$3*3=9$条路径：$$\\frac{\\partial Z}{\\partial X} = \\alpha\\delta + \\alpha\\epsilon + \\alpha\\zeta + \\beta\\delta + \\beta\\epsilon + \\beta\\zeta + \\gamma\\delta + \\gamma\\epsilon + \\gamma\\zeta$$而且当图变复杂时路径数量很容易指数性增长；因此更好的方法是将它们因子分解：$$\\frac{\\partial Z}{\\partial X} = (\\alpha+\\beta+\\gamma)(\\delta+\\epsilon+\\zeta)$$这就是“前向模式微分”和“反向模式微分”的由来。它们通过分解路径来高效求和，在每个节点将路径回到一起的合并起来。前向模式积分从图的输入开始并向前移动知道终点；在每个节点上，它将所有进入的路径相加起来；每条这样的路径代表输入影响节点的一个方式；将它们求和就得到节点受输入影响的总的方式。它的导数是： 另一方面，反向模式微分从图的输出开始一直移动到开始。在每个节点上，它将所有源于节点的路径合并。 前向模式微分追踪一个输入如何影响每个节点，反向模式微分则追踪每个节点如何影响输出，也就是说，前向微分应用算子$\\frac{\\partial}{\\partial X}$到每个节点，而反向模式则应用算子$\\frac{\\partial Z}{\\partial}$到每个节点。 4. 计算胜利那为何每个人都关注反向模式微分呢？它有什么优势吗？考虑原始例子： 可以使用前向模式微分从$b$向上；可以得到每个节点相应于$b$的导数。 计算得到了$\\frac{\\partial e}{\\partial b}$，输出对应于于一个输入的导数。 如果使用反向模式从$e$向下呢？这会得到$e$对应于每个节点的导数： 计算得到了$\\frac{\\partial e}{\\partial a}$和$\\frac{\\partial e}{\\partial b}$，输出对应于所有输入的导数。 想象一个有百万输入的函数，前向模式微分需要贯穿百万次图来获得导数，而反向模式微分仅需一次。在训练神经网络时，将损失视为参数的函数，需要计算损失对应于所有参数的导数来使用梯度下降，因此，反向模式微分，也就是神经网络中的反向传播，可以有极大的加速。 5. 结论导数比想象得更廉价，这是这篇文章的主要教训，但事实上它们是非直觉的廉价，而愚蠢的人类不得不反复发觉它。还有其他的吗，当然。 反向传播也是理解导数如何流过一个模型的有用视角。这在推理为何一些模型难以优化时变得极端有益。经典的例子就是循环神经网络中的梯度弥散问题。 最后，有大量的算法教训从这些技术中获得。反向传播和前向模式微分使用一对强大的技巧（线性化和动态规划）来更高效得计算导数。","tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/tags/deep-learning/"},{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/tags/machine-learning/"}]},{"title":"RNN详解（一）","date":"2017-04-10T13:01:57.000Z","path":"2017/04/10/RNN详解1/","text":"RNN即循环神经网络，是在自然语言处理(NLP)和语音识别(SR)中应用非常广泛的一种模型。本文将会介绍RNN的原理以及相应的python代码实现。 1. RNN是什么RNN的思想是利用序列信息。传统神经网络假设所有的输入（和输出）相互独立，但这对很多任务并不成立。若想要预测句子中的下一个单词，最好知道它前面的单词。RNN被称为“循环”是因为它们对序列中的每个元素执行相同的操作，而输出则依赖于前面的运算。可以这样理解：RNN内部有一个“存储器”来获取迄今计算的信息。理论上RNN可以使用任意长度的信息，但实际上被限制到仅向后看几步。典型的RNN看起来这样： 上图展示了RNN展开为一个全网络，即写出整个序列的网络。比如，如果我们关心的句子有5个单词，网络就会展开为一个5层网络，每层一个词。RNN计算的公式为： $x_t$是在时间$t$的输入。比如$x_1$可能是句中第二个词对应的独热(one-hot)向量。 $s_t$是在时间$t$的隐状态。它是网络的“记忆”。$s_t$基于前一个隐状态和当前输入计算：$s_t=f(Ux_t+Ws_{t-1})$，函数$f$通常非线性，比如tanh或ReLU。第一个要计算的隐变量$s_{-1}$通常初始化为全0。 $o_t$是时间$t$的输出。比如要预测一句话中的下个词，输出是一个在辞典上的概率向量：$o_t=softmax(Vs_t)$。 这里需要注意几件事： 可以认为隐状态$s_t​$是网络的记忆。$s_t​$获取所有前面时间发生的内容，输出$o_t​$仅基于时间$t​$的记忆。但在实际中会更复杂一些，因为$s_t​$通常并不能从前面太多步获取信息。 不同于传统的深度神经网络每一层的参数不同，RNN在所有时间步共享参数（上面的$U,V,W$）。这反应了每一步都执行相同的任务，仅仅输入不同，这样就极大地减少了所需学习的参数量。 上面的图表中在每一步都有输出，但根据任务可能并不需要。比如，在预测句子的感情的时候我们仅关注最后的输出。同样，可能也不需要每一个时间步有输入。RNN的主要特征是隐状态。 2. 训练RNN训练RNN也使用后向传播算法，但会有一些变化。因为网络中的参数被所有的时间步共享，每个输出的梯度不仅依赖于当前步的计算，也与前面步骤有关。比如，要计算$t=4$的梯度，可能需要回溯三步并将梯度相加。这被称为依时间反向传播(BPTT)。但因为存在梯度弥散/爆炸问题，用BPTT训练的普通RNN并不能学习大范围的依赖关系，一些专门设计的RNN（像LSTM）就用来解决这些问题。 3. RNN扩展研究者们已经设计了很多精巧的RNN来解决普通RNN的问题。 双向RNN基于时间$t$的输出可能不仅依赖于序列前一个元素，还依赖于下一个元素的想法。比如，要预测序列中丢失的一个词语，你会想要观察左右两侧的上下文。双向RNN很简单，仅是两个RNN互相首尾堆叠。输出基于两个RNN的隐状态计算。 深度（双向）RNN与双向RNN类似，仅仅是在每一步有多层网络。在实践中有更强的学习能力（但也需要很多的训练数据）。 长短项记忆(LSTM)网络目前很流行，它与RNN并无根本上的不同结构，但使用不同的函数计算隐状态。LSTM中的记忆(memory)被称为单元(cell)，可以将其视为以前面状态$h_{t-1}$和当前输入$x_t$为输入的黑盒。这些单元内在决定记忆中保留（或清除）的信息。然后它们将前个状态、当前记忆和输入结合起来。事实证明这种单元可以非常高效地获取长期依赖。通过可以这篇文章了解更多。 接下来用python从头实现一个RNN，并用theano优化。 4. 语言模型我们将会用RNN建立一个语言模型。假设有$m$个单词组成的句子，语言模型可以预测（在给定数据集中）观察到这个句子的概率： $$P(w_1,\\dots,w_m)=\\prod_{i=1}^m P(w_i|w_1,\\dots,w_{i-1})$$即，一个句子的概率就是每个单词在给定其前面单词的概率的乘积。这个模型为何会有效呢？首先，它可以作为一个打分机制；然后，它可以预测一个词在给定其前面词后出现的概率，这样就可以产生新的文本，因此可以作为一个生成式模型，Andrej Karpathy在这方面有一篇很好的文章。 需注意的是上面的公式中每个词的概率都以所有前面的词为条件；但在实际中会限制到仅看前面几个单词。 5. 训练数据与预处理训练语言模型需要文本来学习，可以从谷歌的BigQuery上下载15000条稍长的reddit评论。生成的文本可能听起来像reddit的评论。 文本标记化，我们将以词为单位预测文本。使用nltk的word_tokenize和sent_tokenize函数完成。 剔除稀有词可以显著加快训练的速度，而对于这样的词语也没有足够的上下文样本来学习怎样正确地使用。在代码中会限制辞典到vocabulary_size个常见单词（这里设为8000，但视情况可改变）。将所有不在辞典中的单词替换为UNKNOW_TOKEN。单词UNKNOW_TOKEN也会包含在辞典中，并像其他词一样用于预测。当产生新文本时再将其替换，比如可以随机从不在辞典中的单词抽取，或者直到生成不含未知标签的句子。 添加特殊开始和结束标记可以帮助我们学习哪些词倾向于开始或结束句子。我们添加特殊的SENTENCE_START和SENTENCE_END标记到每个句子。这使我们可以探求：给定首个SENTENCE_START，下一个词可能是什么（实际上的第一个词）？ 建立训练数据矩阵，因RNN的输入是向量，需要产生词到索引之间的映射，index_to_word和word_to_index。比如单词”friendly”可能索引是2001，一个训练样本的$x$可能看起来是$[0,179,341,416]$，其中0对应SENTENCE_START；对应的标签$y$可能是$[179,341,416,1]$。记住我们的目标是预测下一个单词，因此$y$就是$x$向量用SENTENCE_END移动一个位置。也就是说，单词179正确的预测是341，实际的下一个词。 这是我们文本中一个真实的训练样例： 1234567x:SENTENCE_START what are n&apos;t you understanding about this ? ![0, 51, 27, 16, 10, 856, 53, 25, 34, 69] y:what are n&apos;t you understanding about this ? ! SENTENCE_END[51, 27, 16, 10, 856, 53, 25, 34, 69, 1] 6. 构建RNN考虑到矩阵乘法的工作原理，不能使用单词索引作为输出，这里用一个vocabulary_size维的独热向量表示单词，因此输入$x$就是一个矩阵，每个$x_t$是一个向量，这种转变会在神经网络的代码中实现。输出$o$也是同样的格式。每个$o_t$是vocabulary_size个元素的向量，每个元素代表那个词在句子中为下个词的概率。 RNN的公式概括起来是： $$s_t=tanh(Ux_t+Ws_{t-1})$$ $$o_t=softmax(Vs_t)$$ 写下矩阵和向量的维数通常很有用。假设字典长度$C=8000$，隐层规模是$H=100$。隐层可视为网络的记忆，规模越大越利于学习复杂的模式，但也会带来额外的计算。因此就有： $x_t \\in \\mathbb{R}^{8000}$ $o_t \\in \\mathbb{R}^{8000}$ $s_t \\in \\mathbb{R}^{100}$ $U \\in \\mathbb{R}^{100\\times 8000}$ $V \\in \\mathbb{R}^{8000\\times 100}$ $W \\in \\mathbb{R}^{100\\times 100}$ 这里的$x_t,o_t,s_t$都是列向量，需记住的是上面形式指数的第一个元素总是表示行数。 其中$U,V,W$是网络要从数据中学习的参数。因此，总共需要学习$2HC+H^2$个参数，这里是1610000。注意到$x_t$是独热向量，用$U$相乘等价于从$U$中选择一列。因此网络中最大的矩阵乘法就是$Vs_t$。 7. 初始化首先声明一个初始化参数的RNN类。初始化$U,V,W$参数比较棘手，并不能都初始为0，因为这样在网络层中会导致对称计算，必须随机生成。因合适的初始化对训练结果影响很大，所以在此领域已有很多研究。实践证明最佳的初始化依赖于激活函数，推介的方法是在区间$\\left[ -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right]$，其中$n$是来自上一层的接下来的连接的个数。但其实只要用小的随机值来初始化参数，通常效果都不错。 12345678910111213class RNNNumpy: def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4): # Assign instance variables self.word_dim = word_dim self.hidden_dim = hidden_dim self.bptt_truncate = bptt_truncate # Randomly initialize the network parameters self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim)) self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim)) self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim)) 8. 前向传播接下来实现前向传播（预测单词概率）。 123456789101112131415161718def forward_propagation(self, x): # The total number of time steps T = len(x) # During forward propagation s saves all hidden states # Add one additional element for the initial hidden s = np.zeros((T+1, self.hidden_dim)) s[-1] = np.zeros(self.hidden_dim) # The variable o save the output at each time step o = np.zeros((T, self.word_dim)) # For each time step for t in np.arange(T): # Index U by x[t], the same as multiplying U with one-hot vector # 因x[t]是独热向量，所以U与x[t]的内积就相当于选取U中x[t]非零的那一列 s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1])) o[t] = softmax(self.V.dot(s[t])) return [o, s]RNNNumpy.forward_propagation = forward_propagation 不仅返回计算结果，也返回隐状态，后面会使用它们计算梯度。每个$o_t$是表示辞典中刺的概率，预测的下一个次就是概率最高的。 123456def predict(self, x): # Perform forward propagation and return index of highest score o, s = self.forward_propagation(x) return np.argmax(o, axis=1)RNNNumpy.predict = predict 9. 计算损失使用交叉熵损失作为损失函数$L$，目标是找到$U,V,W$为训练数据最小化损失函数。假设有$N$个训练样本（文本中的单词）和$C$类（字典的规模），则预测$o$和真实标签$y$所对应的损失就是：$$L(y,o) = -\\frac{1}{N} \\sum_{n \\in N} y_n \\log o_n$$对应的程序是： 12345678910111213141516171819202122def calculate_total_loss(self, x, y): L = 0 # For each sentence for i in np.arange(len(y)): o, s = self.forward_propagation(x[i]) # 那个o[array, list]的作用是array和list各自对应位置的元素组成二维索引 # 提取o中索引位置的值，共索引len(array)个，也就是得到o在原本应该是正确 # 答案词的位置，作用其实也相当于将正确词对应的独热向量与输出作内积 # 用在后面的测试与正确词原本应该对应的1的距离。 # 优化目标实际上就是使得预测的结果在正确的词上面最大 correct_word_predictions = o[np.arange(len(y[i])), y[i]] # Add to the loss based on how off they were L += -1 * np.sum(np.log(correct_word_predictions)) return Ldef calculate_loss(self, x, y): # Divide the total loss by the number of training examples N = np.sum(len(y_i) for y_i in y) return self.calculate_total_loss(x,y) / NRNNNumpy.calculate_total_loss = calculate_total_lossRNNNumpy.calculate_loss = calculate_loss 10. 依时间梯度下降这里将$o_t$表示为$\\hat{y}_t$，$L(y,o)$表示为$E(y,\\hat{y})$，于是有$$\\begin{aligned}E_t(y_t, \\hat{y}_t)&amp;= -y_{t} \\log \\hat{y}_{t} \\\\E(y, \\hat{y})&amp;=\\sum\\limits_{t} E_t(y_t,\\hat{y}_t) \\\\&amp; = -\\sum\\limits_{t} y_{t} \\log \\hat{y}_{t} \\end{aligned}$$其中$y_t$是当前步的正确单词，$\\hat{y}_t$是我们的预测，通常将整个序列（句子）视为一个训练样本，所以总的错误就是将每个时间步（单词）的错误相加起来。 我们的目标是计算损失对应于参数$U,V,W$的梯度，并通过SGD学习好的参数。对于一个训练样本，我们也将每个时间步的梯度相加$\\frac{\\partial{E}}{\\partial{W}}=\\sum_t \\frac{\\partial{E_t}}{\\partial{W}}$。使用链式法则计算这些梯度，以$E_3$为例：$$\\begin{aligned}\\frac{\\partial{E_3}}{\\partial{V}}&amp;= \\frac{\\partial{E_3}}{\\partial{\\hat{y}_3}} \\frac{\\partial{\\hat{y}_3}}{\\partial{V}} \\\\&amp;= \\frac{\\partial{E_3}}{\\partial{\\hat{y}_3}} \\frac{\\partial{\\hat{y}_3}}{\\partial{z_3}} \\frac{\\partial{z_3}}{\\partial{V}} \\\\&amp;= (\\hat{y}_3 - y_3) \\otimes s_3\\end{aligned}$$其中$z_3=Vs_3$，$\\otimes$表示两个向量的外积，可见$\\frac{\\partial{E_3}}{\\partial{V}}$仅依赖于当前时间步的值：$\\hat{y}_3, y_3, s_3$。但对$\\frac{\\partial{E_3}}{\\partial{W}}$则不同（对$U$而言也是这样）。写下链式规则，有$$\\frac{\\partial{E_3}}{\\partial{W}} = \\frac{\\partial{E_3}}{\\partial{\\hat{y}_3}} \\frac{\\partial{\\hat{y}_3}}{\\partial{s_3}} \\frac{\\partial{s_3}}{\\partial{W}}$$其中$s_3=tanh(Ux_t+Ws_2)$依赖于$s_2$，而$s_2$则依赖于$W$和$s_1$。所以在求$W$的导数时不能简单得将$s_2$取为常数，而需要再次使用链式法则，所以最后得到的是：$$\\begin{aligned}\\frac{\\partial E_3}{\\partial W}&amp;= \\sum\\limits_{k=0}^{3} \\frac{\\partial E_3}{\\partial \\hat{y}_3}\\frac{\\partial\\hat{y}_3}{\\partial s_3}\\frac{\\partial s_3}{\\partial s_k}\\frac{\\partial s_k}{\\partial W}\\\\\\end{aligned}$$将每一步对梯度的贡献相加。换句话说，因为$W$在每一步我们所关注的输出都被用到了，我们需要从$t=3$开始通过网络将梯度自始至终后向传播至$t=0$： 这恰与前向神经网络中标准的反向传播算法一样，关键的不同在于我们将$W$的梯度在每个时间步上相加（前向神经网络则是每一层正向传播，所以一个在时间维度上是深度网络，另一个则是空间维度上的）。传统的神经网络在层之间并不共享参数，所以无需求和。对于展开的循环神经网络，BPTT是标准反向传播的一个非常别致的名称。就像反向传播可以定义一个delta向量来传递后面部分，比如$\\delta_2^{(3)}=\\frac{\\partial{E_3}}{\\partial{z_2}}=\\frac{\\partial{E_3}}{\\partial{s_3}}\\frac{\\partial{s_3}}{\\partial{s_2}}\\frac{\\partial{s_2}}{\\partial{z_2}}$，其中$z_2=Ux_2+Ws_1$，同样的公式也适用。 11. 用SGD和BPTT训练RNN寻找最小化训练数据上总损失的参数$U,V,W$最通用的方法是SGD，也就是随机梯度下降。已经有很多研究探索优化SGD的方法，包括批处理、并行和自适应学习率。这里只实现一个简单的SGD。 这里使用依时间后向传播(BPTT)算法来计算梯度。因为参数被神经网络的所有时间步所共享，因此每个输出的梯度不仅取决于当前步，还有前面的步骤，使用链式规则来计算。 12345678910111213141516171819202122232425def bptt(self, x, y): T = len(y) # Perform forward propagation o, s = self.forward_propagation(x) # Accumulate the gradients in these variables dLdU = np.zeros(self.U.shape) dLdV = np.zeros(self.V.shape) dLdW = np.zeros(self.W.shape) delta_o = o delta_o[np.arange(len(y)), y] -= 1. # For each output backwards... for t in np.arange(T)[::-1]: dLdV += np.outer(delta_o[t], s[t].T) # Initial delta calculation delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t]**2)) # Backpropagation through time for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]: print \"Backpropagation step t=%d bptt step=%d\" % (t, bptt_step) dLdW += np.outer(delta_t, s[bptt_step-1]) dLdU[:, x[bptt_step]] += delta_t # Update delta for next step delta_t = self.W.T.dot(delta_t) * (1-s[bptt_step-1]**2) return [dLdU, dLdV, dLdW]RNNNumpy.bptt = bptt 12. 梯度检验无论何时实施反向传播，最好也实施梯度验证，那是核实你的实现是否正确的一种方法，背后的思想是参数的导数等于在此点的斜率，这个可以通过稍许改变参数然后用改变去除得到：$$\\frac{\\partial{L}}{\\partial{\\theta}} \\approx \\lim_{h \\to 0}\\frac{J(\\theta+h)-J(\\theta-h)}{2h}$$然后比较反向传播得到的梯度和上面式子得到的梯度，相差不大即可；为减少运算代价，最好是在一个小一点的字典模型上验证。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def gradient_check(self, x, y, h=0.001, error_threshold=0.01): # Calculate the gradients using backpropagation. bptt_gradients = model.bptt(x, y) # List of all parameters we want to check. model_parameters = ['U', 'V', 'W'] # Gradient check for each parameter for pidx, pname in enumerate(model_parameters): # Get the actual parameter value from the mode parameter = operator.attrgetter(pname)(self) print \"Performing gradient check for parameter %s with size %d.\" % ( pname, np.prod(parameter.shape)) # Iterate over each element of the parameter matrix it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index # Save the original value so we can reset it later original_value = parameter[ix] # Estimate the gradient using (f(x+h) - f(x-h))/(2*h) parameter[ix] = original_value + h gradplus = model.calculate_total_loss([x],[y]) parameter[ix] = original_value - h gradminus = model.calculate_total_loss([x],[y]) estimated_gradient = (gradplus - gradminus)/(2*h) # Reset parameter to original value parameter[ix] = original_value # The gradient for this parameter calculated using backpropagation backprop_gradient = bptt_gradients[pidx][ix] # calculate The relative error: (|x - y|/(|x| + |y|)) relative_error = np.abs(backprop_gradient - estimated_gradient)/( np.abs(backprop_gradient) + np.abs(estimated_gradient)) # If the error is to large fail the gradient check if relative_error &gt; error_threshold: print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix) print \"+h Loss: %f\" % gradplus print \"-h Loss: %f\" % gradminus print \"Estimated_gradient: %f\" % estimated_gradient print \"Backpropagation gradient: %f\" % backprop_gradient print \"Relative Error: %f\" % relative_error return it.iternext() print \"Gradient check for parameter %s passed.\" % (pname)RNNNumpy.gradient_check = gradient_check# Use a smaller vocabulary size for checking.grad_check_vocab_size = 100np.random.seed(10)model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)model.gradient_check([0,1,2,3], [1,2,3,4]) 13. SGD实现能够计算梯度后，就可以实现SGD了。可以分两步来实现： 一个sdg_step函数计算梯度并执行一个批次的更新。 12345678910# Perform one step SGDdef numpy_sdg_step(self, x, y, learning_rate): # Calculate the gradients dLdU, dLdV, dLdW = self.bptt(x, y) # Change parameters according to gradients and learning rate self.U -= learning_rate * dLdU self.V -= learning_rate * dLdV self.W -= learning_rate * dLdWRNNNumpy.sgd_step = numpy_sdg_step 一个外层循环来迭代训练集并调整学习率。 123456789101112131415161718192021222324252627282930# Outer SGD Lop# - model: The RNN model instance# - X_train: The training data set# - y_train: the training data labels# - learning_rate: Initial lerning rate for SGD# - nepoch: Number of times to iterate through the complete dataset# - evaluate_loss_after: Evaluate the loss after this many epochsdef train_with_sgd(model, X_train, y_train, learning_rate=0.005, npoch=100, evaluate_loss_after=5): # Keep track of the losses to plot them losses = [] num_examples_seen = 0 for epoch in range(nepoch): # Optionally evaluate the loss if(epoch % evaluate_loss_after == 0): loss = model.calculate_loss(X_train, y_train) losses.append((num_examples_seen, loss)) time = datetime.now().strftime('%Y-%m-%d %H:%M:%S') print \"%s: LOss after num_examples_seen=%d epoch=%d: %f\" % ( time,num_examples_seen, epoch, loss) # Adjust the learning rate if loss increases if(len(losses) &gt; 1 and losses[-1][1] &gt; losses[-2][1]): learning_rate = learning_rate * 0.5 print \"Setting learning rate to %f\" % learning_rate sys.stdout.flush() # For each training examples for i in range(len(y_train)): # One SGD step model.sgd_step(X_trian[i], y_train[i], learning_rate) num_examples_seen += 1 但这样的实现执行起来很费时间，有很多种方法来加速代码的运行，包括层次softmax或者增加一个投影层来避免大矩阵乘法。这里使用GPU来加速。 14. 使用Theano和GPU来训练网络15. 生成文本得到模型后就可以使用它来生成新文本： 123456789101112131415161718192021222324def generate_sentence(model): # Start the sentence with the start token new_sentence = [word_to_index[sentence_start_token]] # Repeat unitl get an end token while not new_sentence[-1] == word_to_index[sentence_end_token]: next_word_probs = model.forward_propagation(new_sentence) sampled_word = word_to_index[unknown_token] # Avoid to sample unknown tokens while sampled_word == word_to_index[unknown_token]: samples = np.random.multinomial(1, next_word_probs[-1]) sampled_word = np.argmax(samples) new_sentence.append(sampled_word) sentence_str = [index_to_word[x] for x in new_sentence[1:-1]] return sentence_strnum_sentences = 10senten_min_length = 7for i in range(num_sentences): sent = [] # To get long sentences while len(sent) &lt; senten_min_length: sent = generate_sentence(model) print \" \".join(sent) 模型成功地学习了句法，但并不通顺或者有语法问题。主要是因为普通RNN无法学得相隔几步远的单词之间的依赖。","tags":[{"name":"deep learning","slug":"deep-learning","permalink":"http://yoursite.com/tags/deep-learning/"},{"name":"NLP","slug":"NLP","permalink":"http://yoursite.com/tags/NLP/"}]},{"title":"配置数学公式环境","date":"2017-04-10T12:14:36.000Z","path":"2017/04/10/配置数学公式环境/","text":"要在hexo中用markdown写出数学公式，需要安装mathjax插件，可通过npm直接安装 要在hexo中用markdown写出数学公式，需要安装mathjax的插件。执行下面的代码：1$ npm install hexo-math --save 有些教程中还写了需要在_cimfig.yml文件中配置plugins，但实测不用配置直接就是能用的。下面就可以用latex的语法在markdown文件中协数学公式了：$\\alpha = \\beta + \\gamma$。123$$\\frac&#123;\\partial E\\_3&#125;&#123;\\partial W&#125; = \\sum\\_&#123;k=0&#125;^3 \\frac&#123;\\partial E\\_3&#125;&#123;\\partial \\hat&#123;y&#125;\\-3&#125;$$ 显示为$$\\frac{\\partial E_3}{\\partial W} = \\sum_{k=0}^3 \\frac{\\partial E_3}{\\partial \\hat{y}_3}$$但这样就会在用一些像_这样的特殊符号的时候就会出现问题，比较好的解决方法是：1234567&#123;% math %&#125;\\begin&#123;aligned&#125;\\dot&#123;x&#125; &amp; = \\sigma(y-x) \\\\\\dot&#123;y&#125; &amp; = \\rho x - y - xz \\\\\\dot&#123;z&#125; &amp; = -\\beta z + xy\\end&#123;aligned&#125;&#123;% endmath %&#125; 显示为$$\\begin{aligned} \\dot{x} &amp; = \\sigma(y-x) \\\\ \\dot{y} &amp; = \\rho x - y - xz \\\\ \\dot{z} &amp; = -\\beta z + xy \\end{aligned}$$","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]}]